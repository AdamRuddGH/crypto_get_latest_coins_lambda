# Crypto lambda to get latest snapshot of all coins
(data is from Coingecko's API - thank you coingecko)

## What problem this solves

This is a deployable lambda which will hit get a market snapshot for all coins, then store it in s3.
Due to the API limits in place, it takes a good 2-3 hours to go through each coin for daily record data if you're looking for data about all coins at a specific time (eg. Every day at 00:00:00 UTC )

Coingecko offers a market endpoint which will give us all data at the time of the request, allowing us to get current details of ~10,000 coin within about 1 minute. 

Pros: 
- Fast (~10,000 coins in ~1min)
- all coins 
- runs on a super inexpensive serverless architecture
- designed with compressed storage in mind
- will notify on completion to a discord channel

Cons: 
- Slight drift in times as the call needs to paginate about 50 times
- no historical load functionality


## Key features

- Interacts with CoinGecko's API (compatible with the free tier)
- Standardises content
- Saves as new Line Delimited Json (JSONnd / JSONLD) so Athena can read this
- GZIPs to save space 
- Uploads to S3
- Automatically writes to a hive parititioned structure
- Notifies you via discord webhook once the job completes


## Data Structure:
Each record will come through (with the following fields)
```json
    {
        "id": "joos-protocol",
        "date": "2021-11-17T21:33:00.633Z", 
        "prices": 0.00035073, 
        "market_caps": 0.0, 
        "total_volumes": 17.01
    }
```

## Installation

AWS Setup:
- create a lambda and assign it a role with access to s3 (putObject, ACL)
- Add environment variables listed in the #Environment Variables section
- upload the zip file generated by `create_lambda.sh` via the `upload zip` button
- Create a cloudwatch trigger for when you want to run the job (remember cloudwatch cron patterns have an extra digit!)

Discord Setup:
- in a channel you own (your own created channel), click `Server Settings`
- click `integrations` from the side menu -> `Webhooks`
- Create a new Webhook by pressing the `New webhook` button. You'll be given a webhook url


Lambda setup:

# Environment Variables

S3_BUCKET - the bucket name you want to use (eg. if your bucket is called "by-cool-bucket", then put that there) 
S3_PARENT_PATH - The path you want to write to the bucket (eg. "daily_coins" )
REQUEST_INTERVAL_WAIT_TIME - optional setting. Defines how long to wait between calls. Defaults to 1 (second) to allow us to play with free tier. If you have an analyst tier or higher you can override this with whatever.

When running locally, you can create a `.env` file with the following

```
S3_BUCKET = my-cool-bucket
S3_PARENT_PATH = daily_coins
DISCORD_WEBHOOK = https://<my_discord_webhook_here>
```



### Provisioning Athena correctly to work with the outputs from this file

When provisioning your Athena Table, use the following template:

```

CREATE EXTERNAL TABLE IF NOT EXISTS `coin_analysis`.`all_coin_data_partitioned` (

`id` string,
`prices` double,
`market_caps` double,
`total_volumes` double,
`date` timestamp

) PARTITIONED BY (
year string,
month string,
day string

)

ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'

WITH SERDEPROPERTIES (
'serialization.format' = '1'
) LOCATION 's3://<YOUR_BUCKET_NAME_HERE>/<YOUR_PARENT_FOLDER_HERE>/'
TBLPROPERTIES ('has_encrypted_data'='false');

```

This will partition your table and significantly save on data searched (from !150mb each query to whatever it actually needs)

However to ensure this is working as expected, run this:
```
MSCK REPAIR TABLE all_coin_data_partitioned;
```
where `all_coin_data_partitioned` is your table path you're writing to. 
This will force a refresh of metadata and partition analysis, bringing in new data so you can query it


### other stuff

Command to gzip a json or csv into an athena readible form
`gzip -c 2021-11-19_all_coin_data.json > 2021-11-19_all_coin_data.json.gz`